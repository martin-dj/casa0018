{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNI+B3VSEKTi//HsLxkmAix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-dj/casa0018/blob/main/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylg6X5bdM8pb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "\n",
        "First set up the necessary Python imports and set up Tensor Board which provides a visual output of the training process."
      ],
      "metadata": {
        "id": "krUuPwlrNRAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# TimeseriesGenerator is now located in tensorflow.keras.utils instead of keras.preprocessing.sequence\n",
        "from tensorflow.keras.utils import timeseries_dataset_from_array # Updated import\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "#%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "7it5S6lPNk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data\n",
        "\n",
        "Create sine function data. We’ll use the NumPy linspace to generate x values ranging between 0 and 60 and NumPy sine function to generate sine values to the corresponding x. We also add a linear componenent to the sine data to generate a trend.\n",
        "\n",
        "We then add some noise to the data. Finally, we visualize the data."
      ],
      "metadata": {
        "id": "TO4OU_ouNtYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 60, 601)  # in radians\n",
        "y = np.sin(x) + 0.02*x\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "y += 0.2*np.random.randn(*y.shape) #Add some noise to the data to make it more realistic\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x, y, 'b.')\n",
        "plt.show()\n",
        "\n",
        "# Define a dataframe using x and y values.\n",
        "df = pd.DataFrame(data=y,index=x,columns=['Sine'])\n"
      ],
      "metadata": {
        "id": "wUhsAUMvOYME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data\n",
        "Split the data into train and validate subsets. The train dataset is used for training the model and the validate data set is used to validate the model against unseen data as it undergoes training. Normally there is a third unseen data set used for testing the trained model. However, the time series used here is sufficiently predictable to test 'by eye'.\n",
        "\n",
        "First, we’ll check the length of the data frame and use a fraction of the data to validate our model. Now if we multiply the length of the data frame with val_percent and round the value (as we are using for indexing purpose) we’ll get the index position i.e., val_index. Last, we’ll split the train and validation data using the val_index."
      ],
      "metadata": {
        "id": "Wq-CQRmrOgcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_pecent = 0.16667\n",
        "len(df)*val_pecent\n",
        "val_point = np.round(len(df)*val_pecent)\n",
        "val_index = int(len(df) - val_point)\n",
        "train = df.iloc[:val_index]\n",
        "val = df.iloc[val_index:]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.xlim(0, 60)\n",
        "\n",
        "print(len(df)) # 601\n",
        "print(len(train))\n",
        "print(len(val))\n",
        "\n",
        "plt.plot(train)\n",
        "plt.plot(val)\n"
      ],
      "metadata": {
        "id": "MjGpQ9ETOmoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalise\n",
        "We need to normalise the data in the range 0-1. We use a scaler to determine the max and min of the complete data set and then use the scaler to scale both the training a validation data set"
      ],
      "metadata": {
        "id": "tXW22zNpOt1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df)\n",
        "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "scaled_train = scaler.transform(train)\n",
        "scaled_val = scaler.transform(val)"
      ],
      "metadata": {
        "id": "OF-m4VV1OxNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Generator\n",
        "One problem we’ll face when using time series data is, we must transform the data into sequences of samples with input data and target data before feeding it into the model. We should select the length of the data sequence (window length) in such a way so that the model has an adequate amount of input data to generalize and predict i.e. in this situation we must feed the model at least with one cycle of sine wave values.\n",
        "\n",
        "The model takes the previous 50 data points (one window) as input data and uses it to predict the next point, which is then compared to the actual target value for backpropagation and gradient descent.\n",
        "\n",
        "This process is time-consuming and difficult if we perform this manually, hence we’ll make use of the Keras Timeseries Generator which transforms the data automatically and ready to train models without heavy lifting.\n",
        "\n",
        "We can see that the length of the scaled_train is 501 and the length of the generator is 451(501–50) i.e. if we perform the tuple unpacking of the generator function using X,y as variables, X comprises the 50 data points (training window) and y contains the 51st data point which the model uses for the prediction target. i.e. X0 = values at timesteps 0-49 and y0 = value at timestep 50; X1 = values at timesteps 1-50 and y1 = value at timestep 51; ... X451 = values at timesteps 451-500 and y451 = value at timestep 501.\n",
        "\n",
        "We also create a validation_generator that operates on the validation data set (scaled_val in the code). The validator_generator is not used to train the model. Instead, it is used after each epoch to validate how well the current model fits an **unseen** data set.\n",
        "\n",
        "The batch size indicates how many sequences (windows) are seen by the network before the network weights are updated. A batch size of 1 means the weights are updated after every sequence (window) is input to the network. In this case a batch size of 10 is used, which speeds up training.\n",
        "\n",
        "An epoch is one complete pass through the training data set."
      ],
      "metadata": {
        "id": "Z3mGW_L9PE1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 30 # sequence length - the length of the training window\n",
        "batch_size = 5\n",
        "\n",
        "# Using timeseries_dataset_from_array to create a generator\n",
        "generator = timeseries_dataset_from_array(data=scaled_train, targets=scaled_train, sequence_length=length, batch_size=batch_size)\n",
        "\n",
        "validation_generator = timeseries_dataset_from_array(scaled_val, scaled_val, sequence_length=length, batch_size=batch_size)\n",
        "\n",
        "print(len(scaled_train)) # 501\n",
        "# the generator object has no length attribute\n",
        "# it is possible to determine the length, but this is more involved\n",
        "# the below code will demonstrate how to find the length if required\n",
        "print(len(list(generator.as_numpy_iterator())))"
      ],
      "metadata": {
        "id": "VbIH-0O_PDry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model\n",
        "\n",
        "The code to create a LSTM is similar to that for earlier NNs you have already seen.\n",
        "\n",
        "The variable (n_features) defined stands for the number of features in the training data i.e., as we are dealing with univariate data we’ll only have one feature whereas if we are using multivariate data containing multiple features then we must specify the count of features in our data."
      ],
      "metadata": {
        "id": "Flu-yhlEQbMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 1\n",
        "\n",
        "# Encoder\n",
        "encoder = Sequential()\n",
        "encoder.add(LSTM(length, return_sequences=True, input_shape=(length, n_features)))\n",
        "\n",
        "# Decoder\n",
        "decoder = Sequential()\n",
        "decoder.add(LSTM(length, return_sequences=False, input_shape=(length, length))) # Pass the output shape of the encoder\n",
        "decoder.add(Dense(1))\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "autoencoder.compile(loss='mse', optimizer='adam')"
      ],
      "metadata": {
        "id": "1Dd7a2btQbws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mtPtDaQKTcjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()\n",
        "autoencoder.fit(generator, epochs=100, validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "iiIpmZJ-TcDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Forecasts Using The Validation Data\n",
        "\n",
        "Now let's compare the forecasts of the LSTM model using our validation data set.\n",
        "\n",
        "Again remembering this is not a true test as the validation set was used to validate our models.\n",
        "\n",
        "Before we plot the results we reverse the scaling transformations."
      ],
      "metadata": {
        "id": "HMfQnKgjWQIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(len(val)):\n",
        "  current_pred = autoencoder.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "val['LSTM Predictions'] = true_predictions\n",
        "val.plot(figsize=(12,8))"
      ],
      "metadata": {
        "id": "8HKU6GjAWS9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}