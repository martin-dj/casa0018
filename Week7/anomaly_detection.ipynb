{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2+jDrqktrZNEsI6nYVmAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-dj/casa0018/blob/main/Week7/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "\n",
        "First set up the necessary Python imports and set up Tensor Board which provides a visual output of the training process."
      ],
      "metadata": {
        "id": "krUuPwlrNRAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "#%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "7it5S6lPNk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data\n",
        "\n",
        "Create sine function data. We’ll use the NumPy linspace to generate x values ranging between 0 and 200*Pi (100 cycles) and NumPy sine function to generate sine values to the corresponding x. We subdivide the range into 1000 data points. We also add a linear componenent to the sine data to generate a trend.\n",
        "\n",
        "We then add some noise to the data. Finally, we visualize the data."
      ],
      "metadata": {
        "id": "TO4OU_ouNtYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 200*3.14159, 1001) # in radians, 100 cycles, 10 data points per cycle\n",
        "#y = np.sin(x) + 0.002*x\n",
        "y = np.sin(x)\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "y += 0.1*np.random.randn(*y.shape) #Add some noise to the data to make it more realistic\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x, y, 'b.')\n",
        "plt.show()\n",
        "\n",
        "# Define a dataframe using x and y values.\n",
        "df = pd.DataFrame(data=y,index=x,columns=['Sine'])\n"
      ],
      "metadata": {
        "id": "wUhsAUMvOYME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data\n",
        "Split the data into train and validate subsets. The train dataset is used for training the model and the validate data set is used to validate the model against unseen data as it undergoes training. Normally there is a third unseen data set used for testing the trained model. However, the time series used here is sufficiently predictable to test 'by eye'.\n",
        "\n",
        "First, we’ll check the length of the data frame and use a fraction of the data to validate our model. Now if we multiply the length of the data frame with val_percent and round the value (as we are using for indexing purpose) we’ll get the index position i.e., val_index. Last, we’ll split the train and validation data using the val_index."
      ],
      "metadata": {
        "id": "Wq-CQRmrOgcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_pecent = 0.2\n",
        "len(df)*val_pecent\n",
        "val_point = np.round(len(df)*val_pecent)\n",
        "val_index = int(len(df) - val_point)\n",
        "train = df.iloc[:val_index]\n",
        "val = df.iloc[val_index:]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.xlim(0, 200*3.142)\n",
        "\n",
        "print(len(df)) # 601\n",
        "print(len(train))\n",
        "print(len(val))\n",
        "\n",
        "plt.plot(train)\n",
        "plt.plot(val)\n"
      ],
      "metadata": {
        "id": "MjGpQ9ETOmoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalise\n",
        "We need to normalise the data in the range 0-1. We use a scaler to determine the max and min of the complete data set and then use the scaler to scale both the training a validation data set"
      ],
      "metadata": {
        "id": "tXW22zNpOt1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df)\n",
        "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "scaled_train = scaler.transform(train)\n",
        "scaled_val = scaler.transform(val)"
      ],
      "metadata": {
        "id": "OF-m4VV1OxNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Generator\n",
        "One problem we’ll face when using time series data is, we must transform the data into sequences of samples with input data and target data before feeding it into the model. We should select the length of the data sequence (window length) in such a way so that the model has an adequate amount of input data to generalize and predict i.e. in this situation we must feed the model at least with one cycle of sine wave values.\n",
        "\n",
        "The model takes the previous 20 data points (one window) as input data and uses it to predict the next point, which is then compared to the actual target value for backpropagation and gradient descent.\n",
        "\n",
        "This process is time-consuming and difficult if we perform this manually, hence we’ll make use of the Keras Timeseries Generator which transforms the data automatically and ready to train models without heavy lifting.\n",
        "\n",
        "We can see that the length of the scaled_train is 801 and the length of the generator is 41(801–20) i.e. if we perform the tuple unpacking of the generator function using X,y as variables, X comprises the 20 data points (training window) and y contains the 21st data point which the model uses for the prediction target. i.e. X0 = values at timesteps 0-19 and y0 = value at timestep 20; X1 = values at timesteps 1-20 and y1 = value at timestep 21; ... X781 = values at timesteps 781-800 and y781 = value at timestep 801.\n",
        "\n",
        "We also create a validation_generator that operates on the validation data set (scaled_val in the code). The validator_generator is not used to train the model. Instead, it is used after each epoch to validate how well the current model fits an **unseen** data set.\n",
        "\n",
        "The batch size indicates how many sequences (windows) are seen by the network before the network weights are updated. A batch size of 1 means the weights are updated after every sequence (window) is input to the network. In this case a batch size of 10 is used, which speeds up training.\n",
        "\n",
        "An epoch is one complete pass through the training data set."
      ],
      "metadata": {
        "id": "Z3mGW_L9PE1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 20 # sequence length - the length of the training window\n",
        "batch_size = 10\n",
        "\n",
        "# Using timeseries_dataset_from_array to create a generator\n",
        "generator = timeseries_dataset_from_array(data=scaled_train, targets=scaled_train, sequence_length=length, batch_size=batch_size)\n",
        "\n",
        "validation_generator = timeseries_dataset_from_array(scaled_val, scaled_val, sequence_length=length, batch_size=batch_size)\n",
        "\n",
        "print(len(scaled_train)) # 801\n",
        "# the generator object has no length attribute\n",
        "# it is possible to determine the length, but this is more involved\n",
        "# the below code will demonstrate how to find the length if required\n",
        "print(len(list(generator.as_numpy_iterator())))"
      ],
      "metadata": {
        "id": "VbIH-0O_PDry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model\n",
        "\n",
        "The code to create a LSTM is similar to that for earlier NNs you have already seen.\n",
        "\n",
        "The variable (n_features) defined stands for the number of features in the training data i.e., as we are dealing with univariate data we’ll only have one feature whereas if we are using multivariate data containing multiple features then we must specify the count of features in our data."
      ],
      "metadata": {
        "id": "Flu-yhlEQbMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 1\n",
        "\n",
        "# Encoder\n",
        "encoder = Sequential()\n",
        "encoder.add(LSTM(length, return_sequences=True, input_shape=(length, n_features)))\n",
        "\n",
        "# Decoder\n",
        "decoder = Sequential()\n",
        "decoder.add(LSTM(length, return_sequences=False, input_shape=(length, length))) # Pass the output shape of the encoder\n",
        "decoder.add(Dense(1))\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "1Dd7a2btQbws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mtPtDaQKTcjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(generator, epochs=20, validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "iiIpmZJ-TcDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Forecasts Using The Validation Data\n",
        "\n",
        "Now let's compare the forecasts of the LSTM model using our validation data set.\n",
        "\n",
        "Again remembering this is not a true test as the validation set was used to validate our models.\n",
        "\n",
        "Before we plot the results we reverse the scaling transformations."
      ],
      "metadata": {
        "id": "HMfQnKgjWQIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(len(val)):\n",
        "  current_pred = autoencoder.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  # The issue was with the slicing of current_batch[:,1:,1:] which results in an\n",
        "  # array with shape (1, 19, 0) along dimension 2. We need to keep the dimension 2\n",
        "  # to be 1 in order to append the new prediction which has shape (1,1).\n",
        "  # Instead of current_batch[:,1:,1:], we should use current_batch[:,1:,:] to select\n",
        "  # all rows, columns from index 1 onwards, and all elements along the third dimension\n",
        "  # This results in the correct shape (1, 19, 1) allowing the append to happen without error.\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1) # Changed this line to fix the dimensional error\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "val['LSTM Predictions'] = true_predictions\n",
        "val.plot(figsize=(12,8))"
      ],
      "metadata": {
        "id": "8HKU6GjAWS9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eDTNqyhBBYtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZeSKl30zBhN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zvqt7S2sSJyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iDqtUXqrfH_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8dETtLgSf6-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CwJf7vfk-6Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==== Generate Sinusoidal Data ====\n",
        "def generate_sinusoidal_data(samples=1000, time_steps_in=50, time_steps_out=20, frequency=0.1, noise=0.0):\n",
        "    \"\"\"Generates sinusoidal time series data for training\"\"\"\n",
        "    X, y = [], []\n",
        "    t = np.linspace(0, samples * frequency, samples + time_steps_in + time_steps_out)\n",
        "    data = np.sin(2 * np.pi * t) + noise * np.random.randn(len(t))  # Sin wave with noise\n",
        "\n",
        "    for i in range(samples):\n",
        "        X.append(data[i : i + time_steps_in])  # Past values\n",
        "        y.append(data[i + time_steps_in : i + time_steps_in + time_steps_out])  # Future values\n",
        "\n",
        "    X = np.array(X).reshape(samples, time_steps_in, 1)  # Reshape for LSTM\n",
        "    y = np.array(y).reshape(samples, time_steps_out, 1)  # Reshape for LSTM\n",
        "    return X, y\n",
        "\n",
        "# Generate Data\n",
        "time_steps_in = 50  # Number of past time steps\n",
        "time_steps_out = 20  # Number of future steps to predict\n",
        "num_samples = 1000\n",
        "X, y = generate_sinusoidal_data(num_samples, time_steps_in, time_steps_out)\n",
        "\n",
        "# ==== Encoder-Decoder Model ====\n",
        "latent_dim = 64  # LSTM units\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(time_steps_in, 1))\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]  # Context vector\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(time_steps_out, 1))  # Input: Zeros during training\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(1)  # Predict next step value\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()\n",
        "\n",
        "# ==== Train Model ====\n",
        "decoder_input_data = np.zeros_like(y)  # Zero padding during training (teacher forcing)\n",
        "model.fit([X, decoder_input_data], y, batch_size=16, epochs=30, validation_split=0.1)\n",
        "\n",
        "# ==== Inference (Prediction) ====\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_sequence(input_seq):\n",
        "    \"\"\"Generates a predicted sequence from the trained model\"\"\"\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, time_steps_out, 1))  # Start with zeros\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(time_steps_out):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        predictions.append(output_tokens[:, -1, 0])  # Get last timestep prediction\n",
        "        target_seq[:, -1, 0] = output_tokens[:, -1, 0]  # Update input for next step\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return np.array(predictions).flatten()\n",
        "\n",
        "# ==== Example Prediction ====\n",
        "test_input = X[0].reshape(1, time_steps_in, 1)  # Use the first training example\n",
        "predicted_sequence = predict_sequence(test_input)\n",
        "\n",
        "# ==== Plot Results ====\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(range(time_steps_in), test_input.flatten(), label=\"Input (Past)\")\n",
        "plt.plot(range(time_steps_in, time_steps_in + time_steps_out), y[0].flatten(), label=\"True Future\")\n",
        "plt.plot(range(time_steps_in, time_steps_in + time_steps_out), predicted_sequence, label=\"Predicted Future\", linestyle=\"dashed\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Sinusoidal Time Series Forecasting with LSTM Encoder-Decoder\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BxovTpSJL-Bt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}